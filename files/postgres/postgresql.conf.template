# ---------- CITUS EXTENSION ----------
# ARCHITECTURE:
#   This is the single most important line for a Citus cluster.
#   It loads the Citus extension code into shared memory at startup.
#   Without this, the node functions as a standard, standalone Postgres instance
#   and cannot participate in distributed queries.
shared_preload_libraries = 'citus'

# ---------- NETWORK ----------
# VISIBILITY:
#   Listens on all interfaces (*).
#   SECURITY: Access control is delegated to pg_hba.conf and Kubernetes NetworkPolicies.
#   Restricting listen_addresses inside the container is usually redundant and error-prone in K8s.
listen_addresses = '*'
port = 5432

# ---------- CONNECTIONS ----------
# SCALING:
#   We use environment variable injection (${VAR}) for these values.
#   WHY: Allows the same Docker image to be used for tiny dev instances and massive prod nodes.
#   The values are injected by 'envsubst' in the entrypoint script.
max_connections = ${MAX_CONNECTIONS}

# CITUS REQUIREMENT:
#   MUST be >= max_connections.
#   WHY: Citus uses Two-Phase Commit (2PC) for distributed consistency.
#   Every client connection might initiate a distributed transaction that reserves a slot.
#   If this is lower than max_connections, transactions will fail unpredictably under load.
max_prepared_transactions = ${MAX_PREPARED_TRANSACTIONS}
superuser_reserved_connections = 3

# ---------- MEMORY ----------
# TUNING:
#   shared_buffers: Data caching (typ. 25% RAM).
#   effective_cache_size: OS cache hint for the planner (typ. 75% RAM).
#   work_mem: Per-operation memory (sort/hash). Keeping it moderate (${WORK_MEM}) prevents OOM
#   when many complex queries run in parallel.
shared_buffers = ${SHARED_BUFFERS}
effective_cache_size = ${EFFECTIVE_CACHE_SIZE}
work_mem = ${WORK_MEM}
maintenance_work_mem = ${MAINTENANCE_WORK_MEM}

# ---------- WAL / CHECKPOINT ----------
# PERFORMANCE:
#   'wal_buffers = -1' auto-tunes based on shared_buffers size.
#   'checkpoint_timeout = 15min' is higher than default (5min) to reduce I/O spikes
#   during bulk data loading, which is common in analytical (OLAP) workloads.
wal_buffers = -1
max_wal_size = 12GB
checkpoint_timeout = 15min

# ---------- PLANNER / I/O ----------
# OPTIMIZATION:
#   Assumes SSD storage (random_page_cost = 1.2).
#   If running on HDD (e.g., cheap cloud storage), this should be increased to 4.0
#   to discourage the planner from choosing index scans over sequential scans.
random_page_cost = 1.2
effective_io_concurrency = 64

# ---------- PARALLEL ----------
# CONCURRENCY:
#   Controls how many CPU cores a SINGLE query can consume.
#   Important for Citus, which parallelizes across nodes, but also within nodes.
max_worker_processes = ${MAX_WORKER_PROCESSES}
max_parallel_workers = ${MAX_PARALLEL_WORKERS}
max_parallel_workers_per_gather = 4

# ---------- LOGGING ----------
# OBSERVABILITY:
#   Logs to stderr so Kubernetes/Docker can capture logs (kubectl logs).
#   'log_min_duration_statement = 500ms' acts as a slow query log,
#   helping to identify performance bottlenecks without flooding the logs.
log_destination = 'stderr'
logging_collector = off
log_min_duration_statement = 500ms
log_line_prefix = '%m [%p] %u@%d %r '

# ---------- CITUS REPLICATION ----------
# REPLICATION:
#   'logical' is required for Citus to move shards between nodes (rebalancing)
#   without downtime. Standard 'replica' level is insufficient for shard moves.
wal_level = logical