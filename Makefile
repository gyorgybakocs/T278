# ---------------------------------------------------------------------------------
# ---------------------------- CONFIGURATION --------------------------------------
# ---------------------------------------------------------------------------------
BASE_NAMES = REDIS
DOCKERHUB_BASE_NAMES = PGBOUNCER POSTGRES
PORTFORWARD = PGBOUNCER REDIS POSTGRES
ROLLING_DEPLOYMENTS = PGBOUNCER REDIS POSTGRES-COORDINATOR POSTGRES-WORKER

# Default credentials (injectable via command line)
# SECURITY:
#   These are insecure defaults intended strictly for local development (Minikube).
#   In production, these must be overridden by CI/CD secrets.
REDIS_PASS ?= redissecret
POSTGRES_PASS ?= password
AWS_KEY_ID ?= AKIAX7SUEXAT7OWTXRLH
AWS_SECRET_KEY ?= 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
AWS_REGION ?= eu-central-1
ECR_URL ?= 548858542119.dkr.ecr.eu-central-1.amazonaws.com

# ---------------------------------------------------------------------------------
# ---------------------------- AWS & BASE IMAGES ----------------------------------
# ---------------------------------------------------------------------------------
aws-login:
	@echo "======================= CONFIGURING AWS CLI =========================="
	@aws configure set aws_access_key_id "$(AWS_KEY_ID)"; \
	aws configure set aws_secret_access_key "$(AWS_SECRET_KEY)"; \
	aws configure set region "$(AWS_REGION)";
	@echo "======================= LOGGING IN TO AWS ECR =========================="
	# INTENT:
	#   Authenticate the local Docker daemon with the remote ECR registry to allow
	#   pulling base images that are not public.
	@aws ecr get-login-password --region $(AWS_REGION) | \
	docker login --username AWS --password-stdin "$(ECR_URL)"

base-build:
	@echo "----------------- Starting port-forward to local registry -------------------"
	# TRADE-OFF:
	#   We run port-forward in the background (&) to allow the subsequent docker commands
	#   to push to localhost:5000. This is brittle; if the port-forward dies, the push fails.
	@kubectl port-forward svc/registry 5000:5000 > /dev/null 2>&1 &
	@echo "Waiting for port-forward..." && sleep 5

	@echo "----------------- Pulling, Tagging, and Pushing Base Images -------------------"
	@# These variables are now read from the ConfigMaps generated by Helm
	@IMG_PREFIX="tis"; \
	ENV_TAG="prd"; \
	BITBUCKET_BRANCH="master"; \
	AWS_ECR_REGISTRY_URL="$(ECR_URL)"; \
	for base_name in $(BASE_NAMES); do \
    		config_map=$$(echo $$base_name | tr 'A-Z' 'a-z')-config; \
    		\
    		echo "Reading metadata from ConfigMap: $$config_map"; \
    		IMAGE_NAME=$$(kubectl get configmap $$config_map -o jsonpath="{.data.$${base_name}_IMAGE}"); \
    		VERSION=$$(kubectl get configmap $$config_map -o jsonpath="{.data.$${base_name}_VERSION}"); \
    		LOCAL_TAG=$$(kubectl get configmap $$config_map -o jsonpath="{.data.$${base_name}_BUILT_IMAGE}"); \
    		\
    		ECR_IMAGE_PATH="$${AWS_ECR_REGISTRY_URL}/$${IMG_PREFIX}-$${IMAGE_NAME}:$${ENV_TAG}-$${BITBUCKET_BRANCH}-$${VERSION}"; \
    		LOCAL_IMAGE_PATH="localhost:5000/$${LOCAL_TAG}"; \
    		\
    		echo "--- Processing service: $$base_name ---"; \
    		echo "Pulling from: $${ECR_IMAGE_PATH}"; \
    		docker pull "$${ECR_IMAGE_PATH}"; \
    		echo "Tagging as: $${LOCAL_IMAGE_PATH}"; \
    		docker tag "$${ECR_IMAGE_PATH}" "$${LOCAL_IMAGE_PATH}"; \
    		docker push "$${LOCAL_IMAGE_PATH}"; \
    		echo "--- Done ---"; \
    	done

	@echo "----------------- Pulling, Tagging, and Pushing Docker Hub Base Images -------------------"
	# PURPOSE:
	#   Mirror public images (Docker Hub) into the local registry.
	# WHY:
	#   This ensures the cluster works even if internet access is lost later,
	#   and speeds up pod startup time by keeping images local to the node.
	@for base_name in $(DOCKERHUB_BASE_NAMES); do \
			config_map=$$(echo $$base_name | tr 'A-Z' 'a-z')-config; \
			\
			echo "Reading metadata from ConfigMap: $$config_map"; \
			DOCKERHUB_IMAGE_PATH=$$(kubectl get configmap $$config_map -o jsonpath="{.data.$${base_name}_IMAGE}"); \
			LOCAL_TAG=$$(kubectl get configmap $$config_map -o jsonpath="{.data.$${base_name}_BUILT_IMAGE}"); \
			LOCAL_IMAGE_PATH="localhost:5000/$${LOCAL_TAG}"; \
			\
			echo "--- Processing Docker Hub service: $$base_name ---"; \
			echo "Pulling from: $${DOCKERHUB_IMAGE_PATH}"; \
			docker pull "$${DOCKERHUB_IMAGE_PATH}"; \
			echo "Tagging as: $${LOCAL_IMAGE_PATH}"; \
			docker tag "$${DOCKERHUB_IMAGE_PATH}" "$${LOCAL_IMAGE_PATH}"; \
			docker push "$${LOCAL_IMAGE_PATH}"; \
			echo "--- Done ---"; \
		done

	@echo "----------------- Verifying images in local registry -------------------"
	@curl -s http://localhost:5000/v2/_catalog
	@echo "----------------- Killing port-forward process -------------------"
	# CLEANUP:
	#   Kill the background port-forward process to free port 5000.
	#   We ignore errors (|| true) in case the process already died.
	@fuser -k 5000/tcp > /dev/null 2>&1 || true


# ---------------------------------------------------------------------------------
# ------------------------------ MINIKUBE MANAGEMENT ------------------------------
# ---------------------------------------------------------------------------------
mk-up:
	@echo "----------------- Starting Minikube -------------------"
	# CONFIG:
	#   --insecure-registry: Required to allow Minikube to pull from our
	#   local HTTP (non-HTTPS) registry hosted inside the cluster itself.
	minikube start --insecure-registry="192.168.0.0/16" --force
	minikube config set insecure-registry "registry.default.svc.cluster.local:5000"
	minikube config set insecure-registry "localhost:5000"

mk-setup:
	@echo "----------------- Mounting working directory into Minikube -------------------"
	# PURPOSE:
	#   Copy source code into the Minikube VM.
	# WHY:
	#   The 'Kaniko' build jobs running inside the cluster need access to the
	#   Dockerfiles and source scripts. HostPath mounts are unreliable across drivers,
	#   so we use `docker cp`.
	docker cp . minikube:/workspace

mk-delete:
	-minikube delete

# ---------------------------------------------------------------------------------
# -------------------------------- OPS / VERIFY -----------------------------------
# ---------------------------------------------------------------------------------

verify-builds:
	@chmod +x scripts/verify-builds.sh
	@bash scripts/verify-builds.sh

test-system:
	@chmod +x scripts/test-system.sh
	@bash scripts/test-system.sh

calculate-resources:
	@chmod +x scripts/calculate-resources.sh
	@bash scripts/calculate-resources.sh
	@make checking-vars

#calculate-resources:
#	@chmod +x scripts/calculate-resources.sh
#
#	# ðŸ›‘ 1. PRE-CHECK: Wait for stability before touching anything
#	@echo "â³ Waiting for initial DB stabilization..."
#	@make wait-for-ready
#	@kubectl wait --for=condition=ready pod -l app.kubernetes.io/component=postgres-coordinator --timeout=120s
#	@sleep 5
#
#	# ðŸš€ 2. EXECUTE: Apply new resources (Triggers Rolling Update)
#	@bash scripts/calculate-resources.sh
#
#	# ðŸ›‘ 3. POST-CHECK: STRICTLY WAIT for the Rolling Update to finish
#	@echo "â³ Waiting for StatefulSet Rollout to complete (DNS stabilization)..."
#	@kubectl rollout status statefulset/tis-stack-postgres-worker -n default --timeout=300s
#	@kubectl rollout status deployment/tis-stack-postgres-coordinator -n default --timeout=300s
#
#	# ðŸ›¡ï¸ 4. EXTRA SAFETY: Wait for DNS propagation
#	@echo "ðŸ’¤ Sleeping 10s to let internal DNS cache expire..."
#	@sleep 10
#
#	@kubectl get pods
#	@make checking-vars

checking-vars:
	@chmod +x scripts/checking-vars.sh
	@bash scripts/checking-vars.sh

# ---------------------------------------------------------------------------------
# ----------------------------------- BOOTSTRAP -----------------------------------
# ---------------------------------------------------------------------------------
infra-setup:
	@echo "----------------- Applying Infrastructure (Registry) -------------------"
	kubectl apply -f infra/registry.yaml
	kubectl rollout status deployment registry -n default --timeout=180s

# The main entry point
bootstrap: mk-delete mk-up mk-setup infra-setup helm-install-config aws-login base-build build-k8s
	@echo "==========================================================="
	@echo "ðŸŽ‰ Redis System bootstrapped successfully!"
	@echo "==========================================================="
	@echo ""
	@make verify-builds

# ---------------------------------------------------------------------------------
# ----------------------------------- HELM DEPLOY ---------------------------------
# ---------------------------------------------------------------------------------
helm-install-config:
	@echo "----------------- Deploying TIS Stack CONFIG ONLY -------------------"
	@MINIKUBE_IP=$$(minikube ip); \
	helm upgrade --install tis-stack ./charts/tis-stack \
		--namespace default \
		--set global.registry="$$MINIKUBE_IP:30500" \
		--set global.aws.accessKeyId="$(AWS_KEY_ID)" \
		--set global.aws.secretAccessKey="$(AWS_SECRET_KEY)" \
		--set redis.auth.password="$(REDIS_PASS)" \
		--set postgres.auth.password="$(POSTGRES_PASS)" \
		--set global.onlyConfig=true

deploy-app:
	@echo "----------------- Enabling TIS Stack DEPLOYMENTS -------------------"
	# INTENT:
	#   Second pass of Helm deployment: toggle 'onlyConfig' to false.
	# WHY:
	#   We first deploy configs to allow Kaniko builds to read version metadata.
	#   Once builds are done (in 'build-k8s'), we enable the actual workloads here.
	@helm upgrade tis-stack ./charts/tis-stack \
		--namespace default \
		--reuse-values \
		--set global.onlyConfig=false

helm-uninstall:
	helm uninstall tis-stack || true

helm-template:
	@echo "----------------- Debugging Helm Templates -------------------"
	helm template tis-stack ./charts/tis-stack --debug

# ---------------------------------------------------------------------------------
# ----------------------------------- BUILD JOBS ----------------------------------
# ---------------------------------------------------------------------------------
build-k8s:
	@echo "----------------- Syncing code to Minikube -------------------"
	docker cp . minikube:/workspace

	@echo "----------------- Build jobs for Kubernetes -------------------"
	@for base_name in $(ROLLING_DEPLOYMENTS); do \
		name=$$(echo $$base_name | tr 'A-Z' 'a-z'); \
		\
		echo "----------------- Building $$name for Kubernetes -------------------"; \
		kubectl delete job $$name-build --ignore-not-found=true; \
		kubectl apply -f build/$$name/build-job.yaml; \
		echo "Waiting for $$name build job..."; \
		kubectl wait --for=condition=complete job/$$name-build --timeout=180s || \
		(echo "!!! $$name build failed logs: !!!" && kubectl logs job/$$name-build --follow && exit 1); \
		echo "$$name build completed."; \
		done

# ---------------------------------------------------------------------------------
# ----------------------------------- UTILS ---------------------------------------
# ---------------------------------------------------------------------------------

rollout-restart:
	@echo "----------------- Restarting Workloads (Smart Detection) -------------------"
	# INTENT:
	#   Restart pods to pick up new configurations or images without downtime.
	# LOGIC:
	#   Dynamically determines if a workload is a StatefulSet or a Deployment
	#   by inspecting the OwnerReference of the pods. This allows the script
	#   to be generic across different workload types.
	@for base_name in $(ROLLING_DEPLOYMENTS); do \
		svc=$$(echo $$base_name | tr 'A-Z' 'a-z'); \
		selector="app.kubernetes.io/component=$$svc"; \
		echo "â†’ Inspecting $$base_name..."; \
		POD=$$(kubectl get pod -l $$selector -o jsonpath='{.items[0].metadata.name}' 2>/dev/null); \
		if [ -z "$$POD" ]; then \
			echo "   âŒ No pods found for selector: $$selector"; \
		else \
			OWNER_KIND=$$(kubectl get pod $$POD -o jsonpath='{.metadata.ownerReferences[0].kind}'); \
			OWNER_NAME=$$(kubectl get pod $$POD -o jsonpath='{.metadata.ownerReferences[0].name}'); \
			if [ "$$OWNER_KIND" = "StatefulSet" ]; then \
				echo "   â†³ Detected StatefulSet: $$OWNER_NAME"; \
				kubectl rollout restart statefulset $$OWNER_NAME; \
				echo "   â³ Waiting for rollout..."; \
				kubectl rollout status statefulset $$OWNER_NAME; \
			elif [ "$$OWNER_KIND" = "ReplicaSet" ]; then \
				DEPLOY_NAME=$$(kubectl get rs $$OWNER_NAME -o jsonpath='{.metadata.ownerReferences[0].name}'); \
				echo "   â†³ Detected Deployment: $$DEPLOY_NAME"; \
				kubectl rollout restart deployment $$DEPLOY_NAME; \
				echo "   â³ Waiting for rollout..."; \
				kubectl rollout status deployment $$DEPLOY_NAME; \
			else \
				echo "   âš ï¸  Skipping: Unsupported owner kind ($$OWNER_KIND)"; \
			fi; \
		fi; \
	done
	@make wait-for-ready

wait-for-ready:
	@echo "----------------- Waiting for all pods to be Ready -------------------"
	@for base_name in $(ROLLING_DEPLOYMENTS); do \
		svc=$$(echo $$base_name | tr 'A-Z' 'a-z'); \
		selector="app.kubernetes.io/component=$$svc"; \
		echo "â†’ Waiting for pods with selector: $$selector"; \
		kubectl wait --for=condition=Ready pod -l $$selector --timeout=120s || exit 1; \
	done

port-forward-services:
	@echo "----------------- Starting port-forwards... -------------------"
	@-pkill -f "kubectl port-forward" || true
	@sleep 2
	@# Reads from redis-config ConfigMap (Metadata)
	@PORT=$$(kubectl get configmap redis-config -o jsonpath="{.data.REDIS_PORT}"); \
	REDIRECT_PORT=$$(kubectl get configmap redis-config -o jsonpath="{.data.REDIS_REDIRECT_PORT}"); \
	nohup kubectl port-forward svc/redis "$${REDIRECT_PORT}:$${PORT}" > /dev/null 2>&1 & \
	echo "    DONE: localhost:$${REDIRECT_PORT} -> svc/redis:$${PORT}";

# ---------------------------------------------------------------------------------
# ----------------------------------- UTILS ---------------------------------------
# ---------------------------------------------------------------------------------

k8s: bootstrap deploy-app calculate-resources test-system
